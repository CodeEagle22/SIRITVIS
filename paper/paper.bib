@inproceedings{terragni-etal-2021-octis,
    title = "{OCTIS}: Comparing and Optimizing Topic Models Is Simple!",
    author = "Terragni, Silvia  and
      Fersini, Elisabetta  and
      Galuzzi, Bruno Giovanni  and
      Tropeano, Pietro  and
      Candelieri, Antonio",
    editor = "Gkatzia, Dimitra  and
      Seddah, Djam{\'e}",
    booktitle = "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: System Demonstrations",
    month = apr,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.eacl-demos.31",
    doi = "10.18653/v1/2021.eacl-demos.31",
    pages = "263--270",
    abstract = "In this paper, we present OCTIS, a framework for training, analyzing, and comparing Topic Models, whose optimal hyper-parameters are estimated using a Bayesian Optimization approach. The proposed solution integrates several state-of-the-art topic models and evaluation metrics. These metrics can be targeted as objective by the underlying optimization procedure to determine the best hyper-parameter configuration. OCTIS allows researchers and practitioners to have a fair comparison between topic models of interest, using several benchmark datasets and well-known evaluation metrics, to integrate novel algorithms, and to have an interactive visualization of the results for understanding the behavior of each model. The code is available at the following link: \url{https://github.com/MIND-Lab/OCTIS}.",
}

@inproceedings{10.1145/2484028.2484166,
author = {Mehrotra, Rishabh and Sanner, Scott and Buntine, Wray and Xie, Lexing},
title = {Improving LDA Topic Models for Microblogs via Tweet Pooling and Automatic Labeling},
year = {2013},
isbn = {9781450320344},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2484028.2484166},
doi = {10.1145/2484028.2484166},
abstract = {Twitter, or the world of 140 characters poses serious challenges to the efficacy of topic models on short, messy text. While topic models such as Latent Dirichlet Allocation (LDA) have a long history of successful application to news articles and academic abstracts, they are often less coherent when applied to microblog content like Twitter. In this paper, we investigate methods to improve topics learned from Twitter content without modifying the basic machinery of LDA; we achieve this through various pooling schemes that aggregate tweets in a data preprocessing step for LDA. We empirically establish that a novel method of tweet pooling by hashtags leads to a vast improvement in a variety of measures for topic coherence across three diverse Twitter datasets in comparison to an unmodified LDA baseline and a variety of pooling schemes. An additional contribution of automatic hashtag labeling further improves on the hashtag pooling results for a subset of metrics. Overall, these two novel schemes lead to significantly improved LDA topic models on Twitter content.},
booktitle = {Proceedings of the 36th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {889–892},
numpages = {4},
keywords = {topic modeling, microblogs, lda},
location = {Dublin, Ireland},
series = {SIGIR '13}
}

@misc{srivastava2017autoencoding,
      title={Autoencoding Variational Inference for Topic Models}, 
      author={Akash Srivastava and Charles Sutton},
      year={2017},
      eprint={1703.01488},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@inproceedings{stojanovski2014,
	Author = {Stojanovski, Dario and Dimitrovski, Ivica and Madjarov, Gjorgji},
	Booktitle = {Proceedings of the Data Mining and Data Warehouses},
	Date-Modified = {2020-08-12 17:10:23 +0200},
	Pages = {1--4},
	Title = {Tweetviz: Twitter Data Visualization},
	Year = {2014}}

@article{Kant2020, doi = {10.21105/joss.02507}, url = {https://doi.org/10.21105/joss.02507}, year = {2020}, publisher = {The Open Journal}, volume = {5}, number = {54}, pages = {2507}, author = {Gillian Kant and Christoph Weisser and Benjamin Säfken}, title = {TTLocVis: A Twitter Topic Location Visualization Package}, journal = {Journal of Open Source Software} }

@article{blei2003latent,
  title={Latent Dirichlet Allocation},
  author={Blei, D. M. and Ng, A. Y. and Jordan, M. I.},
  journal={Journal of Machine Learning Research},
  volume={3},
  pages={993--1022},
  year={2003}
}

@software{Mueller_Wordcloud_2023,
author = {Mueller, Andreas C},
month = apr,
title = {{Wordcloud}},
url = {https://github.com/amueller/wordcloud},
version = {1.9.1},
year = {2023}
}

@inproceedings{sievert-shirley-2014-ldavis,
    title = "{LDA}vis: A Method for Visualizing and Interpreting Topics",
    author = "Sievert, Carson  and
      Shirley, Kenneth",
    editor = "Chuang, Jason  and
      Green, Spence  and
      Hearst, Marti  and
      Heer, Jeffrey  and
      Koehn, Philipp",
    booktitle = "Proceedings of the Workshop on Interactive Language Learning, Visualization, and Interfaces",
    month = jun,
    year = "2014",
    address = "Baltimore, Maryland, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W14-3110",
    doi = "10.3115/v1/W14-3110",
    pages = "63--70",
}

@inproceedings{bianchi-etal-2021-cross,
    title = "Cross-Lingual Contextualized Topic Models with Zero-Shot Learning",
    author = "Bianchi, Federico  and
      Terragni, Silvia  and
      Hovy, Dirk  and
      Nozza, Debora  and
      Fersini, Elisabetta",
    booktitle = "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume",
    month = apr,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.eacl-main.143",
    doi = "10.18653/v1/2021.eacl-main.143",
    pages = "1676--1683",
    abstract = "Many data sets (e.g., reviews, forums, news, etc.) exist parallelly in multiple languages. They all cover the same content, but the linguistic differences make it impossible to use traditional, bag-of-word-based topic models. Models have to be either single-language or suffer from a huge, but extremely sparse vocabulary. Both issues can be addressed by transfer learning. In this paper, we introduce a zero-shot cross-lingual topic model. Our model learns topics on one language (here, English), and predicts them for unseen documents in different languages (here, Italian, French, German, and Portuguese). We evaluate the quality of the topic predictions for the same document in different languages. Our results show that the transferred topics are coherent and stable across languages, which suggests exciting future research directions.",
}

@article{albalawi-2020,
	author = {Albalawi, Rania and Yeap, Tet Hin and Benyoucef, Morad},
	journal = {Frontiers in Artificial Intelligence},
	month = {7},
	publisher = {Frontiers Media},
	title = {Using Topic Modeling Methods for Short-Text Data: A Comparative Analysis},
	volume = {3},
	year = {2020},
	doi = {10.3389/frai.2020.00042},
	url = {https://doi.org/10.3389/frai.2020.00042},
}

@misc{abuchmueller,
	author = {Abuchmueller},
	title = {GitHub - Abuchmueller/Twitmo: Collect Twitter Data and Create Topic Models with R},
	url = {https://github.com/abuchmueller/Twitmo},
}

@article{Hutto_Gilbert_2014, 
title={VADER: A Parsimonious Rule-Based Model for Sentiment Analysis of Social Media Text}, volume={8}, url={https://ojs.aaai.org/index.php/ICWSM/article/view/14550}, DOI={10.1609/icwsm.v8i1.14550}, abstractNote={ &lt;p&gt; The inherent nature of social media content poses serious challenges to practical applications of sentiment analysis. We present VADER, a simple rule-based model for general sentiment analysis, and compare its effectiveness to eleven typical state-of-practice benchmarks including LIWC, ANEW, the General Inquirer, SentiWordNet, and machine learning oriented techniques relying on Naive Bayes, Maximum Entropy, and Support Vector Machine (SVM) algorithms. Using a combination of qualitative and quantitative methods, we first construct and empirically validate a gold-standard list of lexical features (along with their associated sentiment intensity measures) which are specifically attuned to sentiment in microblog-like contexts. We then combine these lexical features with consideration for five general rules that embody grammatical and syntactical conventions for expressing and emphasizing sentiment intensity. Interestingly, using our parsimonious rule-based model to assess the sentiment of tweets, we find that VADER outperforms individual human raters (F1 Classification Accuracy = 0.96 and 0.84, respectively), and generalizes more favorably across contexts than any of our benchmarks. &lt;/p&gt; }, number={1}, journal={Proceedings of the International AAAI Conference on Web and Social Media}, author={Hutto, C. and Gilbert, Eric}, year={2014}, month={May}, pages={216-225}
}
